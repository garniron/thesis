
\documentclass[./thesis.tex]{subfiles}

 
\begin{document}


\section{Matrix dressing}

So far we have used the second order perturbation to build a zeroth-order wavefunction $\ket \Psi$ using CISPI, and estimate its distance to the FCI energy with a stochastic estimation of $\EPT$.

In both cases we have computed the interaction between $\ket \Psi$ and the external space, but we have not let $\ket \Psi$ be revised under those interactions. This idea is based on the so-called \Bk approximation proposed by Gershgorn and Shavitt.\cite{Gershgorn_1968}

This can be achieved using the intermediate Hamiltonian theory,\cite{0305-4470-18-5-014} intermediate Hamiltonians being a class of effective Hamiltonians\cite{lindgren1982atomic} where not all roots are exact eigenvalues of the full Hamiltonian.
The principle is to build a so-called intermediate Hamiltonian ${\bf \tilde H}$ which, when diagonalized, yields a wavefunction that takes into account the effect of an external space on the internal space. 


Sticking to the state-specific case, the general principle can be understood as follow. This formulation, as it is limited to the state-specific case, is somewhat different and wishes to be more intuitive than the one presented in the article.
Fully taking into account the external space requires to solve 

\newcommand{\Hzero}{{\bf H}^{(0)}}
\newcommand{\hcpl}{{\bf h}}

\begin{equation}
\begin{pmatrix}
\Hzero &  \hcpl\\ 
^\dagger\hcpl & {\bf H}^{(1)}
\end{pmatrix} \begin{pmatrix}
{\bf c}\\ 
{\bf c}^\alpha
\end{pmatrix}=E\begin{pmatrix}
{\bf c}\\ 
{\bf c}^\alpha
\end{pmatrix}.
\end{equation}

With ${\bf H}^{(0)}$ and ${\bf H}^{(1)}$ the zeroth and first-order Hamiltonians, $\bf h$ the coupling term between zeroth and first order spaces, ${\bf c}$ the coefficients for the zeroth-order space and ${\bf c}^\alpha$ the coefficients for the external space.

This diagonalization is normally not feasible due to the external space being too large. However, if we are willing to neglect the external-to-external and internal-to-external influences - in other words, if we freeze the external space - we can only solve the eigenequations associated with internal determinants. As usual, associating $I$ and $J$ to internal determinants and $\alpha$ to external ones, for line $I$ we have
\begin{equation}
\qty(\Hzero_{II} - E)c_I  + \sum_{J \neq I} {c_J \Hzero_{IJ}} + \sum_\alpha {c_\alpha \hcpl_{I \alpha}} = 0.
\label{eq:eignen_sum}
\end{equation}


Obviously, since we froze the external space, we need some way to esimate ${\bf c}^\alpha$. In the presented paper, we used a perturbative estimation consistently with what we used in CIPSI and $\EPT$ computations. Because of this, whenever $\bf c$ is revised, ${\bf c}^\alpha$ needs to be recomputed as well. This makes \Bk an iterative method.

Because $c_\alpha$ coefficients are frozen, $\sum_\alpha {c_\alpha H_{I \alpha}}$ is merely a constant added to the eigenequation of line $I$. Renaming this term $\kappa_I$, we can rewrite \ref{eq:eignen_sum}
\begin{equation}
\qty(\Hzero_{II} - E + \frac{\kappa_I}{c_I})c_I  + \sum_{J \neq I} {c_J \Hzero_{IJ}} = 0.
\label{eq:eignen_kappa}
\end{equation}

It appears solving this new system of linear equations is equivalent to diagonalizing 
\begin{equation}
{\bf \tilde H} = \Hzero + {\bf \Delta}
\end{equation}
with ${\bf \Delta}$ a diagonal matrix
\begin{equation}
\begin{cases}
{\bf \Delta}_{II}=\frac{\kappa_I}{c_I}\\
{\bf \Delta}_{IJ}=0 & \text{if } I \neq J.
\end{cases}
\end{equation}

Here ${\bf \tilde H}$ is the intermediate Hamiltonian and ${\bf \Delta}$ the so-called \emph{dressing matrix}. Consequently ${\bf \tilde H}$ may be referred to as a \emph{dressed Hamiltonian}. 
Note that the dressing matrix is diagonal because of an arbitrary rewriting of equation \ref{eq:eignen_sum} into equation \ref{eq:eignen_kappa}. We can actually build ${\bf \Delta}$ in any way that fulfills
\begin{equation}
\sum_J c_J {\bf \Delta}_{IJ}  = \kappa_I.
\label{eq:arbitrary_dressing}
\end{equation}

While the choice of which elements of ${\bf \Delta}$ are non-zero is arbitrary, it can be of importance for numerical reasons (in addition to obvious storage reasons). However, because we diagonalize ${\bf \tilde H}$ using a Davidson diagonalization, this is of no concern to us. Indeed, Davidson's diagonalization only requires the knowledge of the diagonal of ${\bf \tilde H}$ and of ${\bf \tilde H} {\bf c}$ 
\begin{equation}
{\bf \tilde H} {\bf c} = \Hzero {\bf c} + {\bf \Delta} {\bf c}.
\end{equation}
Because of \ref{eq:arbitrary_dressing}, we have by construction
\begin{equation}
{\bf \Delta} {\bf c} = {\pmb \kappa}
\end{equation}
with ${\pmb \kappa}$ the vector $\qty(\kappa_1, \ldots, \kappa_{\Ndet})$.
Algorithmically, it boils down to computing ${\pmb \kappa}$, which is more expensive to compute than ${\bf \tilde H}{\bf c}$ needed for Davidson's diagonalization. But unlike ${\bf \tilde H}{\bf c}$ which is too large to be stored, and needs to be re-computed on the fly at each Davidson iteration, ${\pmb \kappa}$ can easily be stored, being only a vector with $\Ndet$ elements.


An improved version to this original idea was proposed by Davidson and co-workers under the name shifted-\Bk,\cite{Nitzsche_1978a, Nitzsche_1978b, Rawlings_1983, Kozlowski_1995, Kozlowski_1994a, Kozlowski_1994b, Kozlowski_1994c} which is the one we implemented. Details about this improvement and on how it can be generalized to a multi-state case are available in the presented article.


\section{Selected configuration interaction dressed by perturbation}

\includepdf[page=2-]{article_sbk}

\includepdf[page=-]{article_sbk_sup}


\section{Implementation}
For clarity, indices will be named depending on what they refer to
\begin{itemize}
\item
$i,j$ refer to samples/generator determinants
\item
$m$ refers to checkpoints
\item
$t$ refers to teeth
\end{itemize}

In some respect, computing the dressing matrix is akin to computing $\EPT$. Matrix dressing can be seen as a sum of elementary dressing matrices $\delta(\alpha)$, each one associated with a particular $\ket \alpha$, just like $\EPT$ is a sum of $\epsilon(\alpha)$. It is possible to pack those elementary matrices together like we packed $\ket \alpha$ together for $\EPT$.
\begin{equation}
\Delta_I = \sum_{\alpha \in \mathcal{A}_I} \delta(\alpha)
\end{equation}
\begin{equation}
\Delta = \sum_{I} \Delta_I
\end{equation}
So as we only need $\tilde{\bf H}{\bf c}$ for Davidson's algorithm, the quantity we sample is 
\begin{equation}
{\pmb \kappa}_I = \Delta_I\, {\bf c} = \sum_{\alpha \in \mathcal{A}_I} \delta(\alpha)\, {\bf c}
\end{equation}

Thus, ${\pmb \kappa}_I$ is a sum over all external determinants, and requires to find connections between those determinants and the internal wavefunction. Presumably, the norm of resulting elementary dressing vectors $\pmb \kappa_I$, will have a norm decreasing like $e_I$. Indeed, 
\begin{equation}
e_I = \frac{{\bf c}^\dagger {\bf \Delta}_I {\bf c}}{{\bf c}^\dagger {\bf c}} = \braket{\bf c}{\pmb \kappa_I}.
\end{equation}
Using the Cauchyâ€“Schwarz inequality
\begin{equation}
e_I \le \sqrt{ \braket{{\pmb \kappa}_I}{{\pmb \kappa}_I} \braket{\bf c}{\bf c} } \le
\norm{{\pmb \kappa}_I}.
\end{equation}

With that in mind, is seems possible, theoretically, to generalize our hybrid stochastic-deterministic PT2 for computing dressing vectors.
However there are a few significant differences.
\begin{itemize}
\item
We were estimating a scalar, now we are estimating a vector. How can we quickly estimate the running error?
\item
In both cases we have $\Ngen$ samples, however in the case of $\EPT$ each sample is a scalar, here each sample is a vector of size $\Ndet$. It is easy to store $\Ngen$ scalars, not to store $\Ngen$ vectors of size $\Ndet$. In addition, these vectors must be transmitted from slave nodes to a master node, creating a potential network bottleneck, scaling with $\Ndet$.
\item
In the case of PT2 (at least in its Epstein-Nesbet version), each connection found only requires an increment of some elements of $P(G_{pq})$. At no point 2 connections need to be known at the same time. This is different for methods implemented with matrix dressing. It is possible that the detail of which variational determinants a $\ket \alpha$ connects to, needs to be known in order to be able to compute $delta_\alpha$.
\end{itemize}

To address the first problem, we decided to compute the error bar for $E_{\Delta}$ the energy contribution of $\Delta$. Our dressed matrix being $H + \Delta$, its energy is
    
\begin{equation}
\frac{\langle \Psi |H + \Delta | \Psi\rangle}{\langle \Psi | \Psi \rangle} = \frac{\langle \Psi |H  | \Psi\rangle}{\langle \Psi | \Psi \rangle} + \frac{\langle \Psi |\Delta | \Psi\rangle}{\langle \Psi | \Psi \rangle} = E_H + E_{\Delta} 
\end{equation}


$E_{\Delta}$ will be estimated the same way as $E_{PT2}$ was, based on individual contributions $e_{\Delta_I}$

\section{Implementational details}

Implementationally speaking, just like the state-specific version requires computing a single ${\pmb \kappa}$ vector, the multi-state version requires a ${\pmb \kappa}_k$ vector to be computed for each desired state $k$. This, in principle, should come with minimal cost, since the loop over states can be set as the innermost one.

\begin{algorithm}[h!]
\ForAll{generators}{
	costly computation to find $\ket I$ and $\kalpha$ \;
	\For{k=1,$N_{st}$}{
		$\kappa_I^k \gets \kappa_I^k + c_\alpha^k H_{I\alpha}$ \;
	}
}
\end{algorithm}


In practice, since the exact computation of ${\pmb \kappa}_k$ is as expensive than that of $\EPT$, we need to use the same hybrid stochastic-deterministic approach. Unfortunately, using state-average coefficients for the sampling did not yield satisfying results, so we have to stick to state-specific sampling and thus compute each ${\pmb \kappa}_k$ individually. The multi-state Davidson diagonalization, however, is done a single time per shifted-\Bk iteration.


\subsection{Storage}

The core idea is that, in a Monte-Carlo scheme, even an "exotic" one like our own, the estimated result has to be a linear combination of all samples. At any point $m$ of the Monte-Carlo computation corresponding to $M_m$ combs having been drawn, we can write our estimated dressing vector $\Delta^m$ as :


\begin{equation}
\Delta^m = \sum_{I} \epsilon^m_{I} \Delta_I
\end{equation}


The values for $\epsilon^m_I$ have no dependence on those of $\Delta_I$. They only depend on what samples have been drawn so far. Since we decide beforehand which combs are going to be drawn, we can compute $\epsilon$ vector for any point of the Monte-Carlo before any sample has been computed. The values we chose for $M_m$ act as predetermined "checkpoints".

%This allows us to set up "checkpoints" for predetermined values of $n$. These value are referred to as $M_m$ with $M_0 = 0$ and $M_m < M_{m+1}$.
\begin{comment}
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.00\columnwidth]{figures/matrix_dressing/checkpoint}
		\caption{\label{checkpoint}}
		A REFAIRE EN VERTICAL AVEC J[j]
	\end{center}
\end{figure}
\end{comment}


Those checkpoints can be set at any arbitrary point, but they must be determined beforehand and cannot be changed during the computation ; we will only be able to get results at those points.
For checkpoint $m$, we start with a null vector for $\Delta^m$. Each time an elementary vector $\Delta_I$ is computed
we increment it 



\begin{equation}
\Delta^m \gets \Delta^m + \epsilon_I^{m} \Delta_I
\end{equation}


Once this has been done, $\Delta_I$ can be discarded. Indeed, when checkpoint $m$ is reached, $\Delta^{m}$ has its final value, as obviously $\epsilon_I^{m} = 0$ for any $\Delta_I$ sample that hasn't yet been drawn at checkpoint ${m}$.
For convenience, some parameters can be defined as functions of a checkpoint reached

\begin{equation}
\dot t_m
\end{equation}
the first non-deterministic tooth when checkpoint $m$ is reached, and

\begin{equation}
\dot n_0(m) = n_0(\dot t_m)
\end{equation}

the size of the deterministic range when $m$ is reached.


$\epsilon$ is defined as follow

\begin{equation}
\epsilon^m_i = 
\begin{cases}
1 & \text{if } i \leq \dot n_0(m) \\
\frac{W_T \times M_{m,i}}{w_i \times M_m} & \text{if } i > \dot n_0(m)
\end{cases}
\end{equation}


with $M_{m,i}$ the number of times sample $i$ has been drawn at checkpoint $m$.


The memory cost for a checkpoint $m$ is $2 \times \Ndet$ floats, corresponding to the storage of $\epsilon^m$ and $\Delta^m$. This cost is small enough to allow setting up quite a few checkpoints. However, in addition to this memory cost, comes some computational cost. If we set up $N_{cp}$ checkpoints, it implies each time a sample is drawn, we will have, theoretically, to increment $N_{cp}$ vectors of size $\Ndet$. For quicker jobs, this price may not be negligible. It gets worse if, as was the case in our first implementations, a collector is in charge of updating checkpoints for multiple slaves. 


We can drastically reduce the amount of writing required for each sample by rewriting $\Delta^m$.
Firstly, we define $\Delta^{D,t}$ as the total dressing for teeth $\mathcal{T}_t$
\begin{equation}
\Delta^{D,t} = \sum_{I \in \mathcal{T}_t} \Delta_I
\end{equation}

We rewrite $\Delta^m$ as

\begin{equation}
\label{eq:rewrite1}
\Delta^{m} = \sum_{t=0}^{\dot t_m - 1} \Delta^{D,t}+ \frac{1}{M_m} \sum_{I} \gamma^m_{I} \Delta_I
\end{equation}

The second term being $\Delta^m$ minus its deterministic contribution, we can trivially write (defining $\gamma^ 0_I=0$ for convenience)

\begin{equation}
\gamma^m_i = 
\begin{cases}
0 & \text{if } i \leq \dot n_0(m) \vee m=0 \\
\epsilon^m_i \times M_m = \frac{W_T \times M_{m,i}}{w_i} & \text{if } i > \dot n_0(m) \wedge m \neq 0
\end{cases}
\end{equation}

We define

\begin{align}
\Delta^{S,m} = \sum_I (\gamma^m_I - \gamma^{m-1}_I) \Delta_I
\end{align}


We can rewrite the second term of equation \ref{eq:rewrite1}
\begin{equation}
\frac{1}{M_m}\sum_{I} \gamma^m_{I} \Delta_I = \frac{1}{M_m} \sum_{p=1}^m \Delta^{S,p}
\end{equation}

And write the final form or $\Delta^m$ as
\begin{equation}
\Delta^m=\sum_{t=0}^{\dot t_{m}-1} \Delta^{D,t} + \frac{1}{M_m} \sum_{p=1}^m \Delta^{S,p}
\end{equation}

The vectors we need to store are $\Delta^{D,t}$ and $\Delta^{S,m}$. Each time we compute an elementary dressing vector $\Delta_I$, the need for update goes as follow

\begin{itemize}
\item
$\Delta^{D,t}$ with $I \in \mathcal{T}_t$. This is exactly one write.
\item
$\Delta^{S,m}$ where $\gamma^m_I - \gamma^{m-1}_I \neq 0$. This is
 \begin{itemize}
 \item
 	No write if $I$ is moved to $\mathcal{D}$ in the same checkpoint it is first drawn or computed for tooth filling
 \item
 	Otherwise, one write per checkpoint in which it is drawn until the one where it is moved to $\mathcal{D}$, inclusive.
 \end{itemize}
 While this increases the theoretical maximum of writes to $N_{cp}+1$ per sample, it is much lower in practice.
\end{itemize}

For convenience we define

\begin{equation}
\tilde \epsilon^m_I = \gamma^m_I - \gamma^{m-1}_I
\end{equation}


The task queue is built in a fashion similar to the one for $\EPT$ computation, with some differences.
\begin{itemize}
\item
$M_{m,I}$ are evaluated for computation of $\tilde \epsilon^m_I$.
\item
Instead of $R$ we evaluate $R^{-1}$, based on indices of checkpoints rather than of combs. In the algorithm for $\EPT$, when the first $j$ tasks are completed, comb of index $R[j]$ is available. Here, when the first $R^{-1}[m]$ tasks have been completed, checkpoint $m$ has just become computable.
\item
We create sample subsets $\mathcal{P}_m$ associated with checkpoints. $I \in \mathcal{P}_m$ iff
\begin{equation}
R^{-1}[m-1] < j \leq R^{-1}[m]  \;; R^{-1}[0] = 0
\end{equation}
with $j$ the task index associated with $I$, i.e. $J[j]=I$. These sets are tracked using an array $P$ so that $P[I]=m$ iff $I \in \mathcal{P}_m$.

\end{itemize}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.7\columnwidth]{figures/matrix_dressing/taskqueue}
		\caption{A REFAIRE NOTATION.............}
		\label{fig:eici2}
		$e_I$
	\end{center}
\end{figure}

\begin{algorithm}
	\caption{PRECOMPUTE\_MONTECARLO}
	\label{PRECOMPUTE_MONTECARLO}
	\tcc{Computes task queue}
		\KwData{$M_m$ the desired number of combs for checkpoints $m$. $M_0 = 0$ for convenience. $M_{n+1} > M_n$, $\exists M_n > \Ngen$}
		
		$\tilde M$ array size $\Ndet$ initialized with $0$\;
		$u$ array size $\Ngen$ initialized with random numbers $\in [0,1)_\mathbb{R}$ \;
		
		$R^{-1}[0] \gets 0$ \;
		$N_c \gets 0$ \;
		$N_j \gets n_0(1)$ \;
		\For{$i=1,N_j$}{
			$d[i] \gets TRUE$ \;
			$J[i] \gets i$ \;
		}
		
		$N_{cp} \gets 0$ \;
		$F \gets N_j+1$ \;
		\While{$N_j < \Ngen$}{
			ADD\_COMB shown as algorithm \ref{alg:FILL_TOOTH} \;
			FILL\_TOOTH shown as algorithm \ref{alg:ADD_COMB} \;
			\If{$M_{N_{cp}+1} = N_c$}{
				$N_{cp} \gets N_{cp}+1$ \;
				$R^{-1}[N_{cp}] \gets N_j$ \;
				 %$cpthreshold[m] \gets N_j$ \;
				$M_{N_{cp},*} \gets \tilde M$ \;
			} 
		}
		\If{$R^{-1}[N_{cp}] \neq N_j$}{
			\tcc{adds a final checkpoint}
			$N_{cp} \gets N_{cp}+1$ \;
			$R^{-1}[N_{cp}] = N_j$ \;
		}
\end{algorithm}

%An extra array $P[i]=m$ is built, mapping a sample index $i$ to the checkpoint $m$ in which it is first needed ; in other words, when sample $i$ needs to be evalutated, checkpoints up to $m-1$ have been fully built.

\begin{algorithm}
	\For{$m=1,N_{cp}$}{
	\For{$i=R^{-1}[m-1]+1,R^{-1}[m]$}{
		$P[J[i]] \gets m$ \;
	}
	}
\end{algorithm}

\begin{algorithm}
	\caption{OPTIMIZE\_MONTECARLO}
	\label{alg:OPTIMIZE_MONTECARLO}
	
	%\SetKwFunction{FMain}{OPTIMIZE\_MONTECARLO}
	%\SetKwProg{Fn}{Function}{:}{}
	
	%\Fn(\tcc*[h]{Optional algorithm to reorder jobs so checkpoints are reached as fast as possible.}){\FMain{some args}}{
		%$lastNj \gets 1$ \;
		\For{$m=1,N_{cp}$}{
			$Nmoved \gets 0$ \;
			$firstTask \gets R^{-1}[m-1]+1$ \;
			\For{$j=firstTask,R^{-1}[m]$}{
				\If{$M_{m,J[j]} = 0 \wedge J[j] > \dot n_0(m)$}{
					\tcc{ensures moved jobs are at the end of the checkpoint once sorted}
					$J[j] \gets J[j] + \Ngen$ \;
					$Nmoved \gets Nmoved + 1$ \;
				}
			}
			sort array J from $firstTask$ to $R^{-1}[m]$, inclusive \;
			\tcc{moved jobs are sent to the next checkpoint}
			$R^{-1}[m] \gets R^{-1}[m] - Nmoved$ \;
			\tcc{restores moved jobs original value}
			\For{$j=R^{-1}[m]+1,R^{-1}[m] + Nmoved$}{
				$J[j] \gets J[j] - \Ngen$ \;	
			}
		}
		%$k \gets 1$ \;		
		%\For{$i=2,N_{cp}$}{
	%		\If{$R^{-1}[k] \neq R^{-1}[i]$}{
	%			$k \gets k + 1$ \;	
%				$R^{-1}[k] \gets R^{-1}[i]$ \;
%			}		%
%		}
%		$N_{cp} \gets k$ \;
\end{algorithm}

Algorithm \ref{alg:OPTIMIZE_MONTECARLO} is an optional reorganization of jobs order. It ensures results for a checkpoint are available as quickly as possible, without altering them. Some checkpoints may however be removed if no extra computation is required to reach the next one ; this situation can't happen in the unoptimized task queue, because of the tooth filling. Two things are taken into consideration:
\begin{itemize}
\item
Because no result is available between two checkpoints, the order in which jobs are processed between two checkpoints is irrelevant for the result. So, as is usually the case with parallel jobs, we would like to do the longest tasks first, so that we don't get an extra delay due to a massive task being done last. Therefore, jobs should always be in ascending order ( descending computational time) between two checkpoints.
\item
Because of the "tooth filling", sometimes samples computed inside a checkpoint aren't useful for the result. Since tooth filling picks the first non-computed jobs, they are of high computational cost. The algorithm iterates over checkpoints in ascending order, each time moving such sample to the next checkpoint. Thus, every sample is moved to the first checkpoint it's actually involved in, either deterministically or stochastically.
\end{itemize}



\begin{algorithm}
	\label{COMPUTE_EPSILON}
	\caption{COMPUTE\_EPSILON}
		\KwResult{ $\tilde \epsilon$}
		$\tilde \epsilon^*_* \gets 0$ \;
		$F \gets n_0(1)+1$ \;		
		\For{$m=1,N_{cp}$}{
			\For{$i=R^{-1}[m-1]+1,R^{-1}[m]$}{
				$d[J[i]] \gets true$ \;			
			}
			\While{$d(U+1)$}{
				$U \gets U+1$ \;
		  	}
		  	
		  	$\dot t_m \gets N_{teeth}+1$ \;
			$\dot n_0(m) \gets \Ngen$ \;
			
			\For{$t=2,N_{teeth}+1$}{
				\If{$U < n_0(t)$}{% rTeethI[t]$}{
					$\dot t_m \gets t-1$ \;
					$\dot n_0(m) \gets n_0(t-1)$ \;
					break loop \;
				}
			}
			
			
			%\For{$t=\dot t_m,N_{teeth}$}{
				
				\For{$i=\dot n_0(m)+1, \Ngen$}{
					$\tilde \epsilon^m_i \gets \frac{W_T \times M_{m,i}}{w[i]}$ \;
				}
			%}
		}
		\For{$m=N_{cp},2,step=-1$}{
			$\tilde \epsilon^m_* \gets \tilde \epsilon^m_* - \tilde \epsilon^{m-1}_*$ \;
		}
\end{algorithm}


\begin{algorithm}
	\label{COMPUTE_EPSILON}
	\caption{COMPUTE\_EPSILON}
		\KwResult{ $\tilde \epsilon$}
		$\tilde \epsilon^*_* \gets 0$ \;
		$F \gets n_0(1)+1$ \;		
		\For{$m=1,N_{cp}$}{
			\While{$F \leq \Ngen$}{
				\uIf{$M_{m,F} \neq 0$}{
					$F \gets F+1$ \;
				}\Else{
					break \;
				}
		  	}
		  	$\dot t_m \gets N_{teeth}+1$ \;
			$\dot n_0(m) \gets \Ngen$ \;
			\For{$t=2,N_{teeth}+1$}{
				\If{$F \leq n_0(t)$}{% rTeethI[t]$}{
					$\dot t_m \gets t-1$ \;
					$\dot n_0(m) \gets n_0(t-1)$ \;
					break loop \;
				}
			}
			
			
			\For{$t=\dot t_m,N_{teeth}$}{
				
				\For{$i=n_0(t)+1,n_0(t+1)$}{
					$\tilde \epsilon^m_i \gets \frac{W_T \times M_{m,i}}{w[i]}$ \;
				}
			}
		}
		\For{$m=N_{cp},2,step=-1$}{
			$\tilde \epsilon^m_* \gets \tilde \epsilon^m_* - \tilde \epsilon^{m-1}_*$ \;
		}
\end{algorithm}

\newcommand{\cpsent}{\text{cp\_sent}}
\newcommand{\cpdone}{\text{cp\_done}}
\newcommand{\cpmax}{\text{cp\_done}}
\newcommand{\willsend}{\text{will\_send}}

\begin{algorithm}
	\tcc{code for a slave node, OMP parallel}
	
	cp\_done, cp\_sent : shared scalars initialized to $0$ \;
	cp\_max : shared array size $N_{proc}$ initialized to $0$ \;
	$f$ : shared array size $\Ngen$ initialized to $0$ \;
	
	\tcc{loop for each core $iproc$}
	\While{$\cpdone > \cpsent \vee m < N_{cp}+1$ }{
		try to get task $(I,s)$ from queue \;
		\uIf{task was available}{
			$m$ so that $I \in \mathcal{P}_m$ \;	
		}\Else{
			$m \gets N_{cp} + 1$ \;		
		}
		will\_send $\gets 0$ \;
		--OMP CRITICAL-- \;
		$\cpmax[iproc] \gets m$ \;
		$\cpdone \gets min(\text{cp\_max}) -1$ \;
		\If{$\cpdone > \cpsent$}{
			$\cpsent \gets \cpsent + 1$ \;		
			$\willsend \gets \cpsent$ \;				
		}
		--END OMP CRITICAL-- \;
		\If{$\willsend \neq 0$}{
			SEND $\Delta^{\willsend}$ shown in algo TBD
		}
		\If{$m < N_{cp}+1$}{
			UPDATE with I shown in algo TBD \;
		}
	}
\end{algorithm}


\begin{algorithm}
	\tcc{UPDATE}
	\KwData{global shared scope : $\Delta^{D,t}$ and $\tilde \Delta^{m}$ as described, initialized with $0$}
	\KwData{global shared scope : $f$ an array size $\Ngen$ initialized with $0$. The fragmentation count}
	
	\KwData{from outer scope : $s, I \in \mathcal{P}_m$} 
	\BlankLine
	$t$ so that $I \in \mathcal{T}_t$ \;
	
	compute $\Delta_{I,s}$ \;
	\tcc{lock global arrays before update}
	$\Delta^{D,t} \gets \Delta^{D,t} + \Delta_{I,s}$ \;
	\For{$p=m,N_{cp}$}{
		\If{$\tilde e^p_I \neq 0$}{
			$\Delta^{S,p} \gets \Delta^{S,p} + \tilde \epsilon^{p}_I \Delta_{I,s}$ \;
		}
	}
	$x \gets \Psi \cdot \Delta_{I,s}$ \;
	--ATOMIC-- \;
	$e_I \gets e_I + x$ \;
	$f[I] \gets f[I] + 1$ \;
	--END ATOMIC-- \;
\end{algorithm}


\begin{algorithm}
	\tcc{SEND}
	
	\tcc{compute partial value of $\Delta^m$ with partial values of $\Delta^{D,t}$ and $\Delta^{S,m}$}
	\tcc{sending information for checkpoint $m$}
	
	$\dot f \gets \sum_{I \in \mathcal{P}_{p\leq m}} f[I]$ \;
	\If{$\dot f = 0$}{
		return \;
	}
	
	$\Delta^m \gets 0$ \;
	\For{$p=m,1$}{
		$\Delta^m \gets \Delta^m + \Delta^{S,p}$
	}
	$\Delta^m \gets \frac{\Delta^m}{M_m}$ \; 
	\For{$t=\dot t_m-1,0,-1$}{
		$\Delta^m \gets \Delta^m + \Delta^{D,t}$ \;
	}
	%$\mathcal{I}$ the set of $I \in \mathcal{P}_m$ with $f[I] > 0$ \;
	\tcc{$f$ from the slave algorithm can be used to send $e_{I \in \mathcal{P}_m}$ as a sparse \alert{vector/matrix}}
	send ($m$, $\Delta^m$, $e_{I \in \mathcal{P}_m}$, $\dot f$) \;
\end{algorithm}


\begin{algorithm}
	$m \gets 1$ \;
	$c \gets 1$ \;
	$S$ and $S^{(2)}$ float arrays size $N_{teeth}+1$ initialized with $0$ \;
	$e_* \gets 0$ \;
	$\dot f$ integer array size $N_{cp}$ initialized with $\dot f[m] = \sum_{I \in \mathcal{P}_{p\leq m}} F_{I}$ \;
	\While{}{
		\uIf{$\dot f[m] = 0$}{
			\While{$c \leq M_m$}{
				$S_* \gets S_* + B_*(u[c])$ \;
				$S^{(2)}_* \gets S^{(2)}_* + B_*(u[c])^2$ \;
				$c \gets c+1$ \;
				
			}
			\tcc{$E$ for printing purpose}
			$E \gets \sum_{I \leq \dot{n}_0(m)} e_I + {S_t}/c$ \;
			$t \gets \dot{t}_m$ \;
			error $\gets \sqrt{(S_t^{(2)} - {S_t}^2) (c-1)^{-1} }$ \;
			exit loop if $m=N_{cp}$ \;
			\tcc{chose to not exit if next checkpoint is available}
			exit loop if acceptable error and $f[m+1] \neq 0$\;
			
			$m \gets m+1$ \;
		}
		\Else{
			retrieve $(l, \breve {\Delta}^l$, $\breve {e}_{I \in \mathcal{P}_l},\breve f)$  \;
			increment $e_{I \in \mathcal{P}_l}$ with $\breve e_{I \in \mathcal{P}_l}$\;
			increment $\Delta^l$ with $\breve \Delta^l$ \;
			decrement $\dot f[l]$ with $\breve f$\;
		}
	}
	$\Delta^m$ is the estimated dressing vector \;
	
\end{algorithm}


\section{Conclusion}

In this chapter we have implemented a stochastic version of the shifted-\Bk algorithm, using the same algorithm we used for computation of $\EPT$, and were able to get acceptable accuracies performing a few percent of the full computation.
The additional difficulties of estimating a vector rather than a scalar were solved by setting a limited number of pre-determined checkpoints between which no result is available.

The fact that we implemented the shifted-\Bk method boils down to our choice to build the external space using Epstein-Nesbet perturbative estimation for $c_\alpha$. However, the proposed algorithm do not put any particular restriction on $c_\alpha$. From this stemmed the idea of a more general framework to allow easy experimentation of the effect of different external spaces, which was used to design a \alert{variant} of the MR-CCSD method as shown in chapter \ref{chap:exp_dressing}.


\end{document}
