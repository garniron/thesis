\documentclass[./thesis.tex]{subfiles}

 
\begin{document}

since alpha/alpha interactions are by design ignored, computation of a c\_alpha should only require knowledge of it's connection with internal space \\
SUPER costly done naively \\
list of connection supplied by QP \\
general mode \\
mircolist for connection \\
feuille de PQ to avoid duplicates \\
( custom buffers for more complex cases ) \\
repeated excitation mode(?)  \\
existing excitations are listed \\
alphas are generated through repeating excitations \\

microlisted \\
\section{def}
Idea is to create an external space and compute its influence on our selected space.
External space is only defined by c\_I\_alpha ; user should be able to only define that
By design, intern->extern and extern->extern influence isn't taken into account ( that's why it's called "external space"... )
Take into account c\_I\\alpha in the diagonalization of H : Matrix dressing
Matrix dressing
c\_I\\alpha may be derived from c ; in which case self-consistence/iteration


\section{implementation}
Plein de variable, subsection pour resumer la notation?
For clarity, indices will be named depending on what they refer to
\begin{itemize}
\item
$i,j$ refer to samples/generator determinants
\item
$m$ refers to checkpoints
\item
$t$ refers to teeth
\end{itemize}

In some respect, computing the dressing vector is akin to computing PT2. Matrix dressing can be seen as a sum of elementary dressing vectors $\delta_\alpha$, each one associated with a particular $\ket \alpha$, just like PT2 is a sum of $\epsilon(\alpha)$. It is possible to pack those elementary vectors together like we packed $\ket \alpha$ together for PT2.
$$\Delta_I = \sum_{\alpha \in \mathcal{A}_I} \delta_\alpha$$
$$\Delta = \sum_{I} \Delta_I$$
Thus, both are a sum over all external determinants, and require to find connections between those determinants and the internal wavefunction. Presumably, the norm of resulting elementary dressing vectors, will scale in a fashion similar to that of $E(I)$ ( je sais pu la notation )
With that in mind, is seems possible, theoritically, to generalize our hybrid stochastic-deterministic PT2 for computing dressing vectors.
However there are a few significant difference.
\begin{itemize}
\item
We were estimating a scalar, now we are estimating a vector. What should the error bar refer to?
\item
In both cases we have $N_{gen}$ samples, however for PT2 each sample is a scalar, here each sample is a vector size $N_{det}$. It is easy to store $N_{gen}$ scalars, not to store $N_{gen}$ vectors size $N_{det}$
\item
In the case of PT2 ( at least in it Epstein-Nesbet version ), each connection found, only requires an increment of some elements of $P(G_{pq})$. At no point 2 connections need to be known at the same time. This is different for methods implemented with matrix dressing( or even with other version of PT2? ). It is possible that the detail of which variational determinants a $\ket \alpha$ connects to, needs to be known in ordrer to be able to compute $delta_\alpha$.
\end{itemize}

To address the first problem, we decided to compute the error bar for $E_{\Delta}$ the energy contribution of $\Delta$. Our dressed matrix being $H + \Delta$, its energy is
    $$\frac{\langle \Psi |H + \Delta | \Psi\rangle}{\langle \Psi | \Psi \rangle} = \frac{\langle \Psi |H  | \Psi\rangle}{\langle \Psi | \Psi \rangle} + \frac{\langle \Psi |\Delta | \Psi\rangle}{\langle \Psi | \Psi \rangle} = E_H + E_{\Delta} $$

$E_{\Delta}$ will be estimated just like we did for $E_{PT2}$. A vector size $N_{gen}$ is allocated to store individual $E_{\Delta_I}$.

\subsection{storage}

The core idea is that, in a Monte-Carlo scheme, even an "exotic" one like our own, the estimated result has to be a linear combination of all samples. At any point $m$ of the Monte-Carlo, corresponding to a number of samples being drawn, we can write our estimated dressing vector $\Delta^m$ as :

$$\Delta^m = \sum_{I=1}^{N_{gen}} \epsilon^m_{I} \Delta_I$$

The values for $\epsilon^m_I$ have no dependence on those of $\Delta_I$. They only depend on what samples have been drawn so far. If we decide beforehand in what order samples are to be drawn, we can compute $\epsilon$ vector for any point of the Monte-Carlo before any sample has been computed. This allows us to set up predetermined "checkpoints".

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1.00\columnwidth]{figures/matrix_dressing/checkpoint}
		\caption{\label{checkpoint}}
		CONFUSION ENTRE CHECKPOINT ET DENT POSSIBLE, RENDRE VERTICAL?
	\end{center}
\end{figure}


Those checkpoints can be set at any arbitrary point, but they must be determined beforehand and cannot be changed during the computation ; we will only be able to get results at those points.
For checkpoint $m$, we start with a null vector $\tilde \Delta^m$. Each time an elementary vector $\Delta_I$ is computed
we increment it 


$$\tilde \Delta^m \gets \tilde \Delta^m + \epsilon_I^m \Delta_I$$

Once this has been done, $\Delta_I$ can be discarded. Indeed, when checkpoint $m$ is reached, $\tilde \Delta^m = \Delta^m$, as obviously $\epsilon_I^m = 0$ for any $\Delta_I$ sample that hasn't yet been drawn at $m$. ( preciser que c'est l'ordre de CALCUL pas de TIRAGE ).( VARIABLES DEFINI NORMALEMENT DANS LE CHAPITRE PT2... )

$\epsilon$ is defined as follow

$$acm[i] = \frac{toothSize[toothOfDet[i]]}{w[i]}$$
%$$acw[i] = \frac{teethSize[teethFromDet[i]]}{cm[i]-cm[i-1]};i>1$$


$$\epsilon^m_i = \frac{acw[i] \times n[m,i]}{N[m]}  ; i>rDetI[m] ??$$

$$\epsilon^m_i = 1 ; i \leq rDetI[m] ??$$

The memory cost for a checkpoint $m$ is $2 \times N_{det}$ floats, corresponding to the storage of $\epsilon^m$ and $\tilde \Delta^m$. This cost is small enough to allow setting up quite a few checkpoints. However, in addition to this memory cost, comes some computational cost. If we set up $N_{cp}$ checkpoints, it implies each time a sample is drawn, we will have, theoretically, to incremement $N_{cp}$ vectors of size $N_{det}$. For quicker jobs, this price may not be negligible. It gets worse if, as was the case in our first implementations, a collector is in charge of updating checkpoints for multiple slaves. 


We can drastically reduce the amount of writting required for each sample, by separating the deterministic and stochastic part, and computing differences between checkpoints rather than checkpoints directly.
For the deterministic part, things are straightforward. We allocate a vector $\Delta^{D,t}$ of size $N_{det}$ for each tooth $t$, and one $\Delta^{D,0}$ for the initial deterministic set. For each sample that is computed, the $\Delta^{D,t}$ vector corresponding to the $t$ tooth it belongs in - or $\Delta^{D,0}$ if it belongs in the initial deterministic part - is incremented. That's exactly 1 vector write per sample.

For the stochastic part, instead of using $\epsilon$, we use $\tilde \epsilon$, defined as follow.

$$\tilde \epsilon^m_i = 0 ; i \leq rDetI[m]$$

$$\tilde \epsilon^1_i = acw[1] \times n[1,i]  ; i>rDetI[1] ??$$

$$\tilde \epsilon^m_i = acw[i] \times (n[m,i]-n[m-1,i])  ; i>rDetI[m]; m>1 ??$$

(notation  $\tilde \Delta^m$ utilise aussi pour $\epsilon$ )
As can be seen, $\tilde \Delta^m$ will need to be updated when sample $i$ is computed, only if $(n[m,i]-n[m-1,i]) \neq 0$, in other word, only if $i$ has been drawn between checkpoint $m-1$ and $m$. [This means at most 1 writting ( instead of $N_{cp}$ ) each time a sample is drawn.]FAUX? 

$\epsilon$ can be reconstructed from $\tilde \epsilon$.



$$\epsilon^m_i = \frac{1}{N[m]} \sum_{j=1}^{m} {\tilde \epsilon^j_i} ; i > rDetI[m]$$
$$\epsilon^m_i = 1; i \leq rDetI[m]$$


$\epsilon^m_i$ is not explicitely reconstructed in the shown algorithms, $\Delta^m$ is directly computed from $\tilde \epsilon^m_i$.

FORMULE


\begin{algorithm}
	\caption{OPTIMIZE\_MONTECARLO}
	\label{OPTIMIZE_MONTECARLO}
	
	\SetKwFunction{FMain}{OPTIMIZE\_MONTECARLO}
	\SetKwProg{Fn}{Function}{:}{}
	
	\Fn(\tcc*[h]{Optional algorithm to reorder jobs so checkpoints are reached as fast as possible.}){\FMain{some args}}{
		$lastNj \gets 1$ \;
		\For{$m=1,N_{cp}$}{
			$Nmoved \gets 0$ \;
			\For{$j=lastNj,cpthreshold[m]$}{
				\If{$n[m,J_j] = 0 \& J_j > rDetI[m]$}{
					\tcc{ensures moved jobs are at the end of the checkpoint once sorted}
					$J_j \gets J_j + N_{gen}$ \;
					$Nmoved \gets Nmoved + 1$ \;
				}
			}
			sort array J from $lastNj$ to $cpthreshold[m]$, inclusive \;
			\tcc{moved jobs are sent to the next checkpoint}
			$cpthreshold[m] \gets cpthreshold[m] - Nmoved$ \;
			\tcc{restores moved jobs original value}
			\For{$j=cpthreshold[m]+1,cpthreshold[m] + Nmoved$}{
				$J_j \gets J_j - N_{gen}$ \;	
			}
			$lastNj \gets cpthreshold[m] + 1$ \;
		}
	}
\end{algorithm}

Algorithm (Nmoved) is an optional reorganization of jobs order. It doesn't change the result for any checkpoint, but only ensures it is available as quickly as possible. It takes two things into consideration:
\begin{itemize}
\item
Because no result is available between two checkpoints, the order in which jobs are processed between two checkpoints is irrelevant for the result. So, as is usually the case with parallel jobs, we would like to do the longest tasks first, so that we don't get an extra delay due to a massive task being done last. Therefore, jobs should always be in ascending order ( descending computational time N'A PAS ETE EXPLIQUÃ‰) between two checkpoints.
\item
Because of the "tooth filling", sometimes samples computed inside a checkpoint aren't useful for the result. Since tooth filling picks the first uncomputed jobs, they are of high computational cost. The algorithm iterates over checkpoints in ascending order, each time moving such sample to the next checkpoint. Thus, every sample is moved to the first checkpoint it's actually involved in, either deterministically or stochastically.
\end{itemize}



\begin{algorithm}
	\caption{COMPUTE\_TEETH}
	\label{COMPUTE_TEETH}
	\SetKwFunction{FMain}{COMPUTE\_TEETH}
	\SetKwProg{Fn}{Function}{:}{}
	
	
	\Fn(\tcc*[h]{Computes teeth thresholds, and $rDetI[1]$ the number or samples in the initial deterministic part}){\FMain{$minDetInTeeth$, $N_{teeth}$}}{
		\KwData{$1 \leq minDetInTeeth \leq N_{det}$}
		\KwData{$N_{teeth} \geq 1$}
		
		$rDetI[1] \gets 0$ \;
		$rcw \gets cw[minDetInTeeth]$ \;
		$lcm \gets 0$ \;
		\While{}{
			$teeth\_width = \frac{1 - lcm}{N_{teeth}}$ \;
			\If{$teeth\_width < rcm - lcm$}{
				break \;
			}
			
			$rDetI[1] \gets rDetI[1] + 1$ \;
			\If{$N_{det} - rDetI[1] < minDetInTeeth \times N_{teeth}$}{
				STOP cannot compute with those parameters \;
			}
			$rcw \gets cw[rDetI[1]+minDetInTeeth]$ \;
			$lcw \gets  cw[rDetI[1]]$\;
		}
		$lTeethI[1] = rDetI[1]+1$ \;
		$lTeeth[1] = lcw$ \;
		$rTeeth[1] = lcw + teeth\_width$ \;
		\For{$i=2,N_{teeth}$}{
			$rTeeth[i] = lcw + teeth\_width \times i$ \;
			$lTeeth[i] = rTeeth[i-1]$ \;
		}
		\tcc{ensure there is no numerical precision error}
		$rTeeth[N_{teeth}] \gets 1$ \;
		
		$t \gets 1$\;
		\For{$i=rDetI[1]+1, N_{det}$}{
			\If{$cw[i] \geq rTeeth[t]$}{
				$rTeethI[t] = i$ \;
				$t \gets t + 1$ \;
			}
		}
		$teethSize[1] = cw[rTeethI[1]] - lcw$ \;
		\For{$t=2,N_{teeth}$}{
			$lTeethI[t] \gets rTeethI[t-1] + 1$ \;
			$toothSize[t] = rTeeth[t] - lTeeth[t]$ \;
		}
	}
\end{algorithm}


\begin{algorithm}
	\caption{PRECOMPUTE\_MONTECARLO}
	\label{PRECOMPUTE_MONTECARLO}
	\tcc*[h]{Computes $J$ the array so that $J_i$ is the $i^{th}$ sample that must be computed to perform the Monte-Carlo computation, and $n[m,i]$ the total number of times sample $i$ has been drawn at checkpoint $m$, regardless of which teeth are in the deterministic part.}
	
	\KwData{$cpthreshold[m]$ is the minimal number of jobs required to reach checkpoint $m$. It's updated to match the end of a comb ( pas clair? )}
	\KwData{$cpthreshold[N_{cpmax}] = N_{gen}$}
		$N_s \gets rDetI[1]$ \;
		$N_j \gets rDetI[1]$ \;
		\For{$i=1,N_j$}{
			$d_i \gets TRUE$ \;
			$J_i \gets i$ \;
		}
		
		$curcp \gets 1$ \;
		$curth \gets 1$ \;
		$U \gets N_j$ \;
		\While{$N_j < N_{gen}$}{
		  $ADD\_COMB($,$RANDOM[0,1]$,$J$, $N_j$, $d$, $n_g)$  \;
		  \While{$U < N_{gen}$}{
		    $U \gets U + 1$ \;
            \If{$not\ d_U$}{
              $d_U \gets TRUE$ \;
              $N_j \gets N_j+1$ \;
              $J_{N_j} \gets U$ \;
              break \;
            }
		  }
		  
		  $N_s \gets N_s + 1$ \;
		  $oldcurth \gets curth$ \;
		  \While{$curth \leq N_{cpmax}$}{
		  	\If{$N_j \leq cpthreshold[curth]$}{
		  		$curth \gets curth + 1$ \;
		  	}
		  }
		  
		  \If{$oldcurth \neq curth$}{

		    $cpthreshold[curcp] \gets N_j$ \;
            $n[curcp,*] = n_g[*]$ \;
            $N[curcp] = N_s$ \;
            $curcp \gets curcp+1$ \;	
		  } 
		}
		$N_{cp} = curcp - 1$ \;
	
\end{algorithm}


\begin{algorithm}
	\label{ADD_COMB}
	\caption{ADD\_COMB}
	\SetKwFunction{FAddComb}{ADD\_COMB}
	\SetKwProg{Fn}{Function}{:}{}
	
	\Fn(\tcc*[h]{Add a comb to the Monte-Carlo by updating $N_{jobs}$, $J$ and $d$}){\FAddComb{$rand, N_{jobs}, J, d,m$}}{
		\KwData{$rand \in [0, 1[$ value associated with the comb to be added}
		\KwData{$N_j$ : number of unique samples computed so far.}
		\KwData{$J_i$ : index of the $i^{th}$ unique sample computed. Will be used as the list of jobs for the actual computation.}
		\KwData{$d_i=TRUE$ if sample $i$ has been computed, $FALSE$ otherwise.}
		\KwData{$m$ : index of current checkpoint}
		
		%$rand \gets RANDOM(0,1)$ \;
		\For{$t=1,N_{teeth}$}{
			$v=lTeeth[t] + rand \times toothSize[t]$ \;
			$i=FIND\_SAMPLE(rand, lTeethI[t], rTeethI[t])$ \;
			$n(curcp, i) += 1$ \;			
			\If{$not\ d_i$}{
				$N_{jobs} \gets N_{jobs} + 1$ \;
				$J_{N_{jobs}} \gets i$ \;
				$d_i \gets TRUE$ \;
			}
		}
	}
\end{algorithm}


\begin{algorithm}
\label{FIND_SAMPLE}
\caption{FIND\_SAMPLE}
\SetKwFunction{FFind}{FIND\_SAMPLE}
	\SetKwProg{Fn}{Function}{:}{}
	
	\Fn(\tcc*[h]{Finds sample index associated with drawing random value $v$. An initial range $[l,r]$ is given.}){\FFind{$v$,$l$,$r$}}{
		\KwData{$cw[l-1] \leq v < cw[r] ; cw[0] = 0$}
		\KwResult{Returns $i$ so that $cw[i-1] \leq v < cw[i]; cw[0] = 0$}
		\uIf{$v < cw[l]$}{
			\KwRet $l$ \;
		}\ElseIf{$l-r \leq 1$}{
			\KwRet $r$ \;
		}
		\While{$r-l > 1$}{
			$i \gets int((r-l) / 2)$ \;
			\uIf{$cw[i] < v$}{
				$l \gets i$ \;
			}
			\Else{
				$r \gets i$ \;
			}
		}
		\KwRet $r$ \;
	}
\end{algorithm}


\begin{algorithm}
	\label{COMPUTE_EPSILON}
	\caption{COMPUTE\_EPSILON}
		%\KwData{ $I$ the bitstring representation of a determinant $D_I$}
		\KwResult{ $\tilde \epsilon$ as described ... quelque part}
		$\epsilon^*_* \gets 0$ \;
		$U \gets 1$ \;		
		\For{$m=1,N_{cp}$}{
			$lastTeeth[m] = 0$ \;
			$rDetI[m] = lTeeth[1]-1$ \;
			\While{$U \leq N_{gen}$}{
				\uIf{$n[m,U] \neq 0$}{
					$U \gets U+1$ \;
				}\Else{
					break \;
				}
		  	}
			\For{$t=N_{teeth},1,step=-1$}{
				\If{$U-1 \leq rTeethI[t]$}{
					$lastTeeth[m] = t$ \;
					$rDetI[m] =rTeethI[t]$ \;
					break loop \;
				}
			}
			
			
			\For{$t=lastTeeth[m]+1,N_{teeth}$}{
				\For{$i=lTeethI[t],rTeethI[t]$}{
					$\tilde \epsilon^m_i \gets \frac{toothSize[t] \times n[m,i]}{w[i]}$ \;
				}
			}
		}
		\For{$m=N_{cp},2,step=-1$}{
			$\tilde \epsilon^m_* \gets \tilde \epsilon^m_* - \tilde \epsilon^{m-1}_*$ \;
		}
\end{algorithm}

\begin{algorithm}
	\label{UPDATE_CHECKPOINT}
	\caption{UPDATE\_CHECKPOINT}
		%\tcc{updates checkpoint $i_{cp}$ - ERREUR SUR $\sigma^2$$}
		
		\SetKwFunction{FUpdateCheckpoints}{UpdateCheckpoints}
		\SetKwProg{Fn}{Function}{:}{}
	
		\Fn(\tcc*[h]{Update checkpoints when a sample is computed}){\FUpdateCheckpoints{}}{
			$contrib \gets \delta_I \cdot C$ \;
			$t \gets teethOfDet[I]$ \;

			

				$\Delta^{D,t} \gets \Delta^{D,t} + \delta_I$ \;

		\For{$m=1,N_{cp}$}{
			$\sigma[m] += n(m,I) \times contrib$ \;
			$\sigma_2[m] += n(m,I) \times contrib^2$ \;
			$\tilde \Delta^{m} \gets \tilde \Delta^{m} + n(m,I) \times \Delta_I$ \;
		}
		}
		
\end{algorithm}
\begin{algorithm}
	\label{COMPUTE_CHECKPOINTS}
	\caption{COMPUTE\_CHECKPOINTS}
		\tcc{result for checkpoint $i_{cp}$}
		
		\SetKwFunction{FComputeCheckpoints}{ComputeCheckpoints}
		\SetKwProg{Fn}{Function}{:}{}
	
		\Fn(\tcc*[h]{Compute the final estimated dressing vector}){\FComputeCheckpoints{some args}}{
			%$\Delta^m \gets \Delta_{deterministic}$\;
			\For{$t=0,lastTeeth[m]$}{
				$\Delta^m \gets \Delta^m + \Delta^{D,t}$ \;
			}
			\For{$m'=1,m$}{
				$\Delta^m \gets \Delta^m + \frac{\tilde \Delta^{m'}}{N[m]}$
			}
		}
\end{algorithm}



\subsection{finding connections - microlists}

For merely enumerating the connections between internal and external space, there are two possible ways.
Internal to external, or External to internal
%\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.5\columnwidth]{figures/matrix_dressing/interactions}
		%\caption{\label{interactions}}
	\end{center}
%\end{figure}

Computation-wise, those two possbilities are very different.

The internal to external approach is pretty straightforward, as it means applying every possible double excitation to all determinants of an arbitrary set $\Psi$ of size $N_{det}$. The only difficulty is to ensure the created $\alpha$ are actually external to $\Psi$. We did this and more in our CIPSI implementation.

The external to internal approach is more difficult as it means finding connections between all determinant $\alpha$ in an arbitrary set size $N_{external}$ - typically a few orders of magnitude greated than $N_{det}$ - and a set of abritrary determinants $\Psi$. 

Unfortunately, we typically need to know all connection for a particular $\alpha$ before we can compute its associated dressing vector, which puts us in the internal to external case.

In our CIPSI algorithm, we are only considering $N_{virt}^2$ $\alpha$ at the same time. We can build, for each of those $\alpha$, a list of connected $I$ that we incrementally build from connections we find by internal-to-external approach.


However, the storage space for the worst-case scenario isn't sustainable. ( a reformuler/clarifier ptet)
$G_{pq}$ being the first batch, and the variational wavefuction being all determinants up to quadruply excited from $G$ except those in the $G_{pq}$ batch. There are $~ N_{virt}^2$ unique $\alpha$ in the batch. For each of them, any of the $~N_{occ}^2 N_{virt}^2$ double excitation will lead to a variational determinant, except if it leads to one of the $N_{virt}^2$ determinant of the batch.
We have $~ N_{virt}^2$ alpha each connected to $~(N_{occ}^2-1) N_{virt}^2$ variational determiants, resulting in a storage space $~N_{occ}^2 N_{virt}^4$.
Even in the more realistic case where half of double excitations involving the holes in the HOMO of the HF determinants are present in the internal space, storage needed for the ionzied generator $a_{\bar{HOMO}} a_{HOMO} HF$ will be $N_{virt}^4 / 4$, which, depending on the system size, may be concerningly or prohibitively high. 

(PREUVE?)

\begin{algorithm}
	\label{BUILD_CONNECTED}
	\caption{BUILD\_CONNECTED}
		\KwData{ ---------}
		\KwResult{ ------------}
        $i_1 = N$ \;   
        $L_{1..i_1} \gets D_{1..{i_1}}$ \;
		\ForAll{$r ; B_{r0}$}{
		\tcc {$B_{r0} = FALSE$ if column entirely tagged}
		  $i_2 = i_1 + N^r$ \;
		  $L_{i_1+1..i_2} \gets D^r_{1..N^r}$ \;
		  \For{$i=1,N_r$}{
		    $T_{D^r_i} \gets FALSE$
		  }
		  \ForAll{$s ; B_{s0}$}{
		    $i_3 = i_2$ \;
		    \For{$i=1,N_s$}{
		      \If{$T_{D^s_i}$}{
		        $i_3 \gets i_3+1$ \;
		        $L_{i_3} \gets D^s_i$ \;
		      }
		    }
		    
		    $i_4 = i_3 + N^{rs}$ \;
		    $L_{i_3+1 .. i_4} \gets D^{rs}_{1..N^{rs}}$ \;
		    \tcc{$L$ is the list of all $I \in \Psi ; EXC(I, a^\dagger_r a^\dagger_r G_{pq} ) \leq 2$}
		  }
		 \For{$i=1,N_s$}{
		    $T_{D^r_i} \gets TRUE$
		  }
		}
\end{algorithm}



\begin{algorithm}
	\label{BUILD_MICROLIST}
	\caption{BUILD\_MICROLIST}
		\KwData{ ------}
		\KwResult{ ------}
        $N \gets 0$ \;
        $N^* \gets 0$ \;   
        $N^{*,*} \gets 0$ \;    
        \ForAll{$I \in \{S - past\} ; f_{G_{pq}}^I \leq 4$}{
          $(P,H) \gets particles\_and\_holes(G_{pq}, I)$ \;
          $p = list\_from\_bitstring(P)$ \;
          
          %$h = LIST_FROM_BITSTRING(H)$
          \uIf{$f_{G_{pq}}^I = 4$ \& $B_{p_1,p_2}$}{
            $i \gets N^{p_1, p_2}+1$ \;
            $N^{p_1, p_2} \gets i$ \;
            $D^{p_1, p_2}_{i} \gets I$ \;
          }
          \uElseIf{$f_{G_{pq}}^I = 3$ \& $B_{p_1}$}{
            $i \gets N^{p_1}+1$ \;
            $N^{p_1} \gets i$ \;
            $D^{p_1}_{i} \gets I$ \;
          }
          \Else{
            $i \gets N+1$ \;
            $N \gets i$ \;
            $D_{i} \gets I$ \;
          }
        }
\end{algorithm}


\section{Alpha-factory (rename?)}


General framework which allows to create methods with minimal effort 

\section{parallel}
just like pt2 stoch, 1 task = 1 generator \\
however, 1 result = 1 vector instead of 1 scalar \\
additional difficulties \\
memory: \\
to avoid N\_det\^2, checkpoints \\
communication \\
4 scalars per checkpoint \\



\end{document}