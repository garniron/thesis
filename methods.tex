 
\documentclass[./thesis.tex]{subfiles}
\begin{document}


Quantum chemistry aims at describing the electronic structure of molecular
systems.  The velocity of the nuclei is considered negligible compared to that
of the electrons (Born-Oppenheimer approximation), and for atoms of the first
rows of the periodic table relativistic effects can be neglected.  In this
context, the model system is a cloud of $N$ electrons and a set of $M$ nuclei
considered punctual, immobile charges. It can be described by solving
Shrödinger's equation for electrons~:
\begin{equation}
 \widehat{H} \Psi({\bf x}_1,\dots,{\bf x}_N) = E_N \Psi_n({\bf x}_1,\dots,{\bf x}_N)
\end{equation}
where $\Psi$ is the electronic wave function, $E$ the associated energy, and $\mathbf{x}_i = (\textbf{r},m_s)$
contains the spatial coordinates of the electrons $\textbf{r}$, as well as a spin variable $m_s$.
$\widehat H$ is the non-relativistic electronic Hamiltonian operator
\begin{equation}
\widehat{H} = \sum_{i=1}^{N} \Big ( -\frac{1}{2} \Delta_i - \sum_{j=1}^M \frac{Z_j}{|{\bf r}_i - {\bf R}_j|} \Big ) + \sum_{i=1}^{N} \sum_{k>i}^{N} \frac{1}{|{\bf r}_i - {\bf r}_k|}
\end{equation}
where ${\bf R}_j$ and $Z_j$ are respectively the spatial coordinate and charge of nucleus $j$.

\section{Slater determinants}

The simplest description of the wave function is the Hartree product. This consists in 
building the product of orthonormal one-electron functions, each function describing the state of
one electron:
\begin{equation}
\Psi_{\text{Hartree}}({\bf x}_1,\dots,{\bf x}_{N})  = \prod_{i=1}^N \phi_i(\textbf{x}_i).
\end{equation}
Because of the fermionic nature of electrons, $\Psi$ must satisfy the condition of being antisymmetric with respect to the permutation of electrons coordinates, which is not verified by the Hartree product.
Antisymmetrizing the Hartree product yields the so-called Slater determinant:
\begin{equation}
\begin{array}{c}
 \Psi({\bf x}_1,\dots,{\bf x}_{N}) = 
\frac{1}{\sqrt{N}!} \left|
 \begin{array}{ccc}
 \phi_1({\bf x}_1) & \dots & \phi_1({\bf x}_{N}) \\
 \vdots              & \ddots &   \vdots             \\
 \phi_{N}({\bf x}_1) & \dots & \phi_N({\bf x}_{N}) \\
 \end{array}
\right| \\ 
\end{array} 
\label{eq:slater1}
\end{equation}
which is the simplest possible antisymmetric wave function.
The functions $\phi_i$ are called \emph{spinorbitals}:
\begin{equation}
\phi_i(\textbf{x}) = \varphi_i(\textbf{r}) \, \sigma_i(m_s)
\end{equation}
where $\varphi_i$ is a spatial function, or \emph{molecular orbital (MO)}, and $\sigma_i$ is a
discrete spin function describing the spin state of the electron ($m_s = \pm \frac{1}{2}$). The spin
function can be either $\alpha(m_s)$ or $\beta(m_s)$ defined as
\begin{align}
\alpha \qty(\frac{1}{2}) = 1  \; &; \;  \alpha \qty(-\frac{1}{2}) = 0   \\ \nonumber
\beta  \qty(\frac{1}{2}) = 0  \; &; \;  \beta  \qty(-\frac{1}{2}) = 1,
\end{align}
and for convenience, one will rewrite
\begin{align}
\phi_i(\textbf{x}) & = \varphi_i(r) \alpha(m_s) \\ \nonumber
\overline{\phi}_i(\textbf{x}) & = \varphi_i(r) \beta(m_s).
\end{align}

Packing together the $\alpha$ spinorbitals, and then the $\beta$ spinorbitals in the representation of
the determinant, one can express the Slater determinant as
\begin{equation}
\frac{1}{\sqrt{N}!} \left|
 \begin{array}{cccccc}
 \phi_1({\bf x}_1)  & \dots & \phi_1({\bf x}_{\Nalpha}) & \phi_1({\bf x}_{\Nalpha+1}) & \dots & \phi_1({\bf x}_{N}) \\
 \vdots             & \ddots& \vdots                    & \vdots                      &\ddots & \vdots  \\         
 \phi_{\Nalpha}({\bf x}_1)  & \dots & \phi_{\Nalpha}({\bf x}_{\Nalpha}) & \phi_{\Nalpha}({\bf x}_{\Nalpha+1}) & \dots & \phi_{\Nalpha}({\bf x}_{N}) \\
 \overline{\phi}_{\Nalpha+1}({\bf x}_1)  & \dots & \overline{\phi}_{\Nalpha+1}({\bf x}_{\Nalpha}) & \overline{\phi}_{\Nalpha+1}({\bf x}_{\Nalpha+1}) & \dots & \overline{\phi}_{\Nalpha+1}({\bf x}_{N}) \\
 \vdots             & \ddots& \vdots                    & \vdots                      &\ddots & \vdots  \\         
 \overline{\phi}_{N}({\bf x}_1)  & \dots & \overline{\phi}_{N}({\bf x}_{\Nalpha}) & \overline{\phi}_{N}({\bf x}_{\Nalpha+1}) & \dots & \overline{\phi}_{N}({\bf x}_{N}) \\
 \end{array}
\right| 
\end{equation}
where $\Nalpha$ is the number of $\alpha$ electrons, i.e. the number of electrons with $m_s=1/2$.
If one chooses the permutation in which the first $\Nalpha$ electrons have $m_s=1/2$, and the other electrons have $m_s=-1/2$, one always has
\begin{align}
\phi_i(\mathbf{x}_j) & = 0 \; \text{ for } 1 \le i \le \Nalpha \text{ and } \Nalpha < j \le N \\ \nonumber
\overline{\phi}_i(\mathbf{x}_j) & = 0 \; \text{ for }  \Nalpha < i \le N \text{ and }  1 \le j \le \Nalpha,
\end{align}
and the Slater determinant becomes block-diagonal:
\begin{equation}
\frac{1}{\sqrt{N}!} \left|
 \begin{array}{cccccc}
 \phi_1({\bf x}_1)  & \dots & \phi_1({\bf x}_{\Nalpha}) &   & &   \\
 \vdots             &\ddots & \vdots                    & &  0    & \\         
 \phi_{\Nalpha}({\bf x}_1)  & \dots & \phi_{\Nalpha}({\bf x}_{\Nalpha}) &   & &   \\
   & & & \overline{\phi}_{\Nalpha+1}({\bf x}_{\Nalpha+1}) & \dots & \overline{\phi}_{\Nalpha+1}({\bf x}_{N}) \\
 & 0 & & \vdots                      &\ddots & \vdots  \\         
   & &   & \overline{\phi}_{N}({\bf x}_{\Nalpha+1}) & \dots & \overline{\phi}_{N}({\bf x}_{N}) \\
 \end{array}
\right|. 
\end{equation}
This allows us to rewrite the Slater determinant in a spin-free formalism as
the \emph{Waller-Hartree double determinant},\cite{Pauncz_1989} namely the
product of two determinants associated with $\alpha$ and $\beta$ electrons
respectively.
\begin{equation}
\begin{array}{c}
 \Psi({\bf r}_1,\dots,{\bf r}_{\Na},{\bf r}_{\Na+1},\dots,{\bf r}_N;
      \alpha_1,\dots,\alpha_{\Na},\beta_{\Na+1},\dots,\beta_N) = \\
\frac{1}{\sqrt{N!}} D_\alpha({\bf r}_1,\dots,{\bf r}_{\Na}) \times D_\beta({\bf r}_{\Na+1},\dots,{\bf r}_N) = \\
\frac{1}{\sqrt{N!}} \left|
 \begin{array}{ccc}
 \varphi_1({\bf r}_1) & \dots & \varphi_1({\bf r}_{\Na}) \\
 \vdots               & \ddots &   \vdots             \\
 \varphi_{\Na}({\bf r}_1) & \dots & \varphi_{\Na}({\bf r}_{\Na}) \\
 \end{array}
\right|
\left|
 \begin{array}{ccc}
 \varphi_1({\bf r}_{\Na+1}) & \dots & \varphi_1({\bf r}_{N}) \\
 \vdots               & \ddots &   \vdots             \\
 \varphi_{N_\beta}({\bf r}_{\Na+1}) & \dots & \varphi_{N_\beta}({\bf r}_{N}) \\
 \end{array}
\right| \\ 
\end{array} \\
\label{eq:slater}
\end{equation}

Molecular orbitals are typically defined as linear combinations of \emph{atomic orbitals}, or \emph{AO}, here noted $\chi_k$
\begin{equation}
 \varphi_i({\bf r}) = \sum_k C_{ki} \chi_k({\bf r}).
\label{eq:def_mo}
\end{equation}
These functions qualify the used \emph{basis set}, and are usually themselves pre-defined linear combinations of Gaussian functions. This is a restriction put on the form of the wavefunction, therefore it is known as the \emph{finite basis set approximation}.
In the Hartree-Fock method, the wave function is a single Slater determinant, where the $C_{ki}$ coefficients associated with molecular orbitals are optimized so as to minimize the energy.
This method, however is missing some important physical effects. For instance, using Eq.\eqref{eq:slater} one can see that in this model opposite-spin electrons are statistically independent (or \emph{uncorrelated}):
\begin{equation}
\begin{split}
\left[ \Psi_\textrm{HF}({\bf r}_1,\dots,{\bf r}_{\Na},{\bf r}_{\Na+1},\dots,{\bf r}_N;
      \alpha_1,\dots,\alpha_{\Na},\beta_{\Na+1},\dots,\beta_N) \right]^2 = \\
\left[ D_\alpha({\bf r}_1,\dots,{\bf r}_{\Na}) \times D_\beta({\bf r}_{\Na+1},\dots,{\bf r}_N) \right]^2 = \\
\left[ D_\alpha({\bf r}_1,\dots,{\bf r}_{\Na}) \right]^2 \times \left[ D_\beta({\bf r}_{\Na+1},\dots,{\bf r}_N) \right]^2.
\end{split}
\end{equation}

\section{Electron correlation}

Electron correlation is defined as\cite{Lowdin_1959}
\begin{equation}
\Ecor = E_\text{exact} - E_\text{HF}
\end{equation}
where $E_\text{HF}$ is the \emph{Hartree-Fock limit}, i.e. the limit to which the Hartree-Fock energy converges when the size of the one-electron basis set increases.

To include electron correlation effects, $\Psi$ may be expanded in
$\{\ket D_i \}$, the set of all the possible Slater determinants that can be built by putting $N_\alpha$ electrons in $M$ orbitals and $N_\beta$ electrons in $M$ orbitals.
The eigenvectors of $\widehat{H}$ are consequently expressed as linear combinations of Slater determinants 
\begin{equation}
\ket{\Psi_n} = \sum_{i} c_i^n \ket{D_i}.
\end{equation}
Solving the eigenvalue equations in this basis is referred to as \emph{Full Configurations Interaction (FCI)} and yields solutions for Schrödinger's equation that are exact for the given atomic basis set.
But the FCI is usually computationally intractable because of its scaling with the size of the basis set. Indeed, the size of the FCI space is
\begin{equation}
\NFCI = \frac{M!}{\Nalpha ! (M-\Nalpha)!} \times \frac{M!}{\Nbeta ! (M-\Nbeta)!}.
\end{equation}
Post-Hartree-Fock methods are trying to circumvent this problem, and therefore
are essentially approximations of FCI.

\section{Matrix elements}
In the $N$-electron basis of Slater determinants, one expects the matrix elements of $\widehat H$ to be integrals over $3N$ dimensions.
However, given the two-electron nature of the Hamiltonian, and because the set of molecular orbitals is orthogonal, Slater determinants that differ by more than two electrons yield a zero matrix element, and the other elements can be expressed as sums of integrals over 3 or 6 spatial dimensions, which can be computed for a reasonable cost. These simplifications are known as \emph{Slater-Condon's rules}:

\begin{align}
\Hij{D}{D} & = \sum_{i\in D} \mel{i}{\hat{h}}{i} + \frac{1}{2} \sum_{i\in D} \sum_{j\in D} \Big [ (ii|jj) - (ij|ij) \Big ]      \\
\Hij{D}{D_p^r} & = \mel{p}{\hat{h}}{r} + \sum_{i\in D} \Big [ (pr|ii) - (pi|ri) \Big ]        \\
\Hij{D}{D_{pq}^{rs}} & = (pr|qs) - (ps|qr)
\end{align}
where $\hat{h}$ is the one-electron part of the Hamiltonian (kinetic energy and
electron-nucleus potential),
\begin{equation}
\mel{p}{\hat{h}}{r} = \int d{\bf x}\; \phi^*_p(\mathbf{x}) \qty( -\frac{1}{2}\nabla + V_1(\mathbf{x})) \phi_h(\mathbf{x}),
\end{equation}
$i \in D$ means that $\phi_i$ belongs to the
Slater determinant $D$, $\ket {D_{pq}^{rs}}$ is a determinant obtained from $\ket
D$ by substituting orbitals $\phi_p$ and $\phi_q$ by $\phi_r$ and
$\phi_s$, and
\begin{equation}
(ij|kl) = \int d{\bf x}_1 \int d{\bf x}_2 \; \phi^*_i({\bf x}_1)\phi_j({\bf x}_1) \frac{1}{|{\bf r}_1 - {\bf r}_2|} \phi^*_k({\bf x}_2)\phi_l({\bf x}_2).
\end{equation}

\section{Two-electron integrals}

In the Hartree-Fock method, Roothaan's equations allow to solve the problem in the basis of 
\emph{atomic} orbitals.\cite{Roothaan_1951}
In this context, one needs to compute the $\order{M^4}$ two-electron integrals $(pq|rs)$ over the
atomic orbitals. Thanks to a large effort in algorithmic development and
implementation,\cite{Obara_1986,Head_Gordon_1988,Ten_no_1993,Gill_1989,Gill_1991,Libint2,Zhang_2018}
these integrals can now be computed very fast on modern computers.

But for post-Hartree-Fock methods, the computation of the two-electron
integrals is a potential bottleneck.
Indeed, when computing matrix elements of the Hamiltonian in the basis of
Slater determinants, integrals over \emph{molecular} orbitals are desired. Using
Eq.\eqref{eq:def_mo},
the cost of computing a single integral scales as $\order{M^4}$:
\begin{equation}
(ij|kl) = \sum_{pqrs} C_{pi} C_{qj} C_{rk} C_{sl} (pq|rs)
\end{equation}

A naive computation of all integrals in the MO basis would cost ${\cal O}(M^8)$. Fortunately, computing all of them can be scaled down to ${\cal O}(N^5)$ by transforming the indices one by one:\cite{Wilson_1987}
\begin{eqnarray}
(iq\vert rs) & = & \sum_{p} C_{pi} (pq|rs) \nonumber \\ 
(iq\vert ks) & = & \sum_{r} C_{rk} (iq|rs) \;\; \text{semi-transformed integrals} \\
(ij\vert ks) & = & \sum_{q} C_{qj} (iq|ks) \nonumber \\ 
(ij\vert kl) & = & \sum_{s} C_{sl} (ij|ks)  \;\; \text{fully transformed integrals}
\end{eqnarray}
This step is known as the \emph{four-index integral transformation}.  
In addition to being very costly, this step is hard to parallelize in
distributed way, because it implies multiple collective communications.\cite{Rajbhandari_2017,Limaye_1994,Fletcher_1999,Covick_1990}



\section{Variational post-Hartree-Fock methods}

In variational methods, one tries to minimize the \emph{variational energy}
\begin{equation}
\Evar = \frac{\mel{\Psi}{\hat{H}}{\Psi}}{\braket{\Psi}{\Psi}} \ge \EFCI
\label{eq:evar}
\end{equation}
by optimizing the parameters of the wave function.
Generally speaking, solving Schrö\-dinger's equation in a basis of Slater determinants is called \emph{Configuration Interaction (CI)}.
In these methods, the molecular orbitals are kept fixed and the variational parameters are the coefficients associated with the Slater determinants.
The general idea of CI methods is to select \textit{a priori} a relevant subset
of Slater determinants in which the CI problem will be solved, the FCI being
the particular case where the whole $\{\ket {D_i} \}$ set is used.

One usual approach is to perform a FCI by allowing excitations from a reference determinant only within a reduced set of molecular orbitals. This is referred to as \emph{Complete Active Space Configuration Interaction (CAS-CI)}. Choosing the CAS orbitals often requires some chemical expertise. The CAS-SCF method minimizes the energy by performing iteratively a CAS-CI and optimizing the molecular orbitals.

Another usual CI approach is to select determinants according to their excitation degree --- by how many occupied orbitals they differ --- with respect to some reference. If the reference is the Hartree-Fock determinant and if only single and double excitations are considered, the method is known as \emph{Configuration Interaction with Single and Double excitations (CISD)}. Alternatively, the reference can be a CAS-CI, in which case it is known as \emph{Multi-Reference Configuration Interaction (MR-CI)}.

Regardless of the method, integrals involving all orbitals implied in at least one Slater determinant need to be computed in order to diagonalize $\widehat{H}$. Therefore CI methods cost is at least $\order{M^5}$, due to the four-index transformation.
In addition, the cost for fully diagonalizing $\widehat{H}$ is $\order{\Ndet^3}$ with $\Ndet$ the number of determinants in the considered subspace, which can be up to a few billion. This usually not feasible, but only the few eigenvectors associated with lowest eigenvalues are typically of interest, so iterative methods can be used. The standard choice in quantum chemistry is to use the \emph{Davidson diagonalization} originally developed by Ernst R. Davidson\cite{Davidson_1975} specifically for CI methods. 


\section{Perturbative methods}

%In perturbative methods, one computes the energy by projection on a reference $\Psi_0$:
%\begin{equation}
%\E = \frac{\mel{\Psi_0}{\hat{H}}{\Psi}}{\braket{\Psi_0}{\Psi}}
%\label{eq:evar}
%\end{equation}
%and one uses the intermediate normalization convention: $\braket{\Psi_0}{\Psi} = 1$.

\alert{Pioche un peu chez Manus pour introduire MP2 et Epstein Nesbet.}



\alert{On voit que pour MP2, on n'a pas besoin de toutes les integrales
bielectroniques: les $\braket{vv}{vv}$ ne sont pas necessaires. Donc ca coute
forcement moins cher que le CI.}

\section{Selected CI methods}

These methods rely on the same principle as the usual CI approaches, except that determinants aren't chosen \textit{a priori} based on an occupation or excitation criterion, but selected \emph{on the fly} among the entire set of determinant based on their estimated contribution to the FCI wavefunction. Conventional CI methods can be seen as an exact resolution of Schrödinger's equation for a complete, well-defined set of determinants (and for a given atomic basis set), while selected CI methods are more of a truncation of the FCI.
The main advantages of these methods compared to the more conventional \textit{a priori} selected ones, are that since the most relevant determinants are considered, they will typically yield a more accurate description of physical phenomena, and a much lower energy for an equivalent number of determinants.
It has been noticed long ago that, even inside a predefined subspace of determinants, only a small number significantly contributes to the wavefunction. Therefore, an \emph{on the fly} selection of determinants is a rather natural idea that has been proposed from the late 60's by Bender and Davidson\cite{Bender_1969} as well as Whitten and Hackmeyer\cite{Whitten_1969} and is still very much under investigation, we can name the very recent \emph{Machine Learning Configuration Interaction (MLCI)}.\cite{1808.05787}

The approach we are using is based on the \emph{Configuration Interaction using a Perturbative Selection (CIPSI)} developped by Huron, Rancurel and Malrieu,\cite{Huron_1973} that iteratively selects \emph{external} determinants $\kalpha$ (determinants which are not present in the wavefunction $\Psi$) using a perturbative criterion
\begin{equation}
e_\alpha = \frac{\Hij{\Psi}{\alpha}^2}{\Evar - H_{\alpha \alpha}}.
\end{equation}
with $\Psi$ the variational wavefunction of energy $E$, and $\alpha$ the external determinant being considered. $\EPT$ an estimation to the electronic correlation missing from the variational energy is also computed


\begin{equation}
\EPT = \sum_{\alpha} e_\alpha
\end{equation}

\begin{equation}
E_{FCI} \approx E + \EPT
\end{equation}

There is however a computational downside. In \textit{a priori} selected methods, the rule by which determinants are selected is known \textit{a priori}, and therefore, one can map a particular determinant to some row or column index.\cite{Knowles_1984} As a consequence, it can be systematically determined to which matrix element of $\widehat{H}$ a two-electron integral contributes. This allows for the implementation of so-called \emph{intergral-driven} methods, that work essentially by iterating over integrals.
On the contrary, in selected methods an explicit list has to be maintained, and there is no immediate way to know whether a determinant has been selected, or what its index is. Consequently, so-called \emph{determinant-driven} approaches will be used, in which iteration is done over determinants rather than integrals. This can be a lot more expensive, since the number of determinants is typically much larger than the number of integrals. The former scales as $\order{M!}$ while the latter scales as $\order{M^4}$ with the number of MOs.
Furthermore, determinant-driven methods require an effective way to compare determinants in order to extract the corresponding excitation operators, and a way to rapidly fetch the associated integrals.

Because of this high computational cost, approximations have been proposed.\cite{Evangelisti_1983} And recently, the \emph{Heat-Bath Configuration Interaction (HCI)} algorithm has taken farther the idea of a more approximate but extremely cheap selection.\cite{Holmes_2016, Sharma_2017} Compared to CIPSI, the selection criterion is simplified to
\begin{equation}
e^{HCI}_\alpha = \max \qty( \qty| c_i \Hij{D_i}{\alpha} | )
\end{equation}
with $D_i$ a determinant from the variational wavefunction and $c_i$ its associated coefficient. This algorithmically allows for a much faster selection.

\section{Monte-Carlo methods}
FCI-QMC is an alternate approach to selection recently proposed in 2009 by Alavi \textit{et al.},\cite{Booth_2009,Booth_2010,Cleland_2010} where walkers spawn from one determinant to a connected one, with a probability that is function of the associated matrix element. The average proportion of walkers on a determinant converges to its coefficient in the FCI wavefunction.

A more ``bruteforce'' approach at stochastic selection is \emph{Monte-Carlo CI (MCCI)},\cite{Greer_1995,Greer_1998} where determinants are randomly added to the variational wavefunction. After diagonalization, the determinants of smaller coefficient are removed, and new random determinants are added.




\end{document}
