\documentclass[./thesis.tex]{subfiles}

 
\begin{document}

The diagonalization of the Hamiltonian is a necessary step in configuration
interaction. Standard diagonalization algorithms scale as $\order{\Ndet^3}$ in
terms of computation, and $\order{\Ndet^2}$ in terms of storage, so the cost is
prohibitive as $\Ndet$ ranges usually between a few million and a few billion.

Fortunately,  not all the spectrum of $\hat H$ is required: only the few lowest
eigenstates are of interest. The Davidson
algorithm\cite{Davidson_1975,Liu_1978,Olsen_1990,Gadea_1994,Crouzeix_1994} is an iterative algorithm
which aims at extracting the first few $\Nst$ lowest eigenstates of a large
matrix. This algorithm reduces the cost of the computation
to $\order{\Nst \Ndet^2}$, and of the storage to $\order{\Nst \Ndet}$.
It is presented as algorithm~\ref{alg:davidson}.

\newcommand{\mU}{\mathbf{U}}
\newcommand{\mS}{\mathbf{S}}
\newcommand{\mW}{\mathbf{W}}
\newcommand{\mR}{\mathbf{R}}
\newcommand{\mh}{\mathbf{h}}
\newcommand{\my}{\mathbf{y}}

\begin{algorithm}
 \caption{Davidson's diagonalization algorithm}
 \label{alg:davidson}
	\SetKwFunction{FMain}{DAVIDSON\_DIAG}
	\SetKwProg{Fn}{Function}{:}{}
\Fn(){\FMain{$\Nst, \mU$}}{
	\KwData{ $\Nst$~: Number of requested states}
	\KwData{ $\Ndet$~: Number of determinants}
	\KwData{ $\mU$~: Guess vectors, $\Ndet \times \Nst$}
	\KwResult{ $\Nst$ lowest eigenvalues eigenvectors of $\mH$ }
\texttt{converged} $\gets \FALSE$ \;
\While{$\neg{\texttt{converged}}$}
{
  Gram-Schmidt orthonormalization of $\mU$ \;
  $\mW \gets \mH\, \mU$ \; 
  $\mh \gets \mU^\dagger\, \mW$ \;
  Diagonalize $\mh$ : eigenvalues $E$ and eigenvectors $\my$ \;
  $\mU' \gets \mU\, \my$ \;
  $\mW' \gets \mW\, \my$ \;
  \For{$k\gets 1,\Nst$}{
    \For{$i\gets 1,\Ndet$} {
      $\mR_{ik}  \gets \frac{E_k \mU_{ik}' - \mW_{ik}'}{\mH_{ii} - E_k}$ \;
    }
  }
  $\texttt{converged} \gets \norm{\mR} < \epsilon$ \;
  $\mU \gets [ \mU, \mR ]$ \;
}
\KwRet{$\mU$}\;
}
\end{algorithm}

\clearpage
\section{The computational bottleneck}

Algorithmically, the expensive part of the Davidson diagonalization is the computation of the matrix product $\mH\, \mathbf{U}$.
Determinants are connected by $\mH$ only if they differ by no more than two
spinorbitals. Therefore, the number of non-zero elements per line of $\mH$ is
equal to the number of single and double excitation operators, namely
$\order{\Nalpha^2 \times (\Norb - \Nalpha)^2}$. As $\mH$ is symmetric, the number
of non-zero elements per column is identical. This makes the $\mH$ matrix very
sparse, but for large basis sets the whole $\mH$ matrix may still not fit in the
memory of a single node, as the number of non-zero entries to store is
$\order{\Ndet \times \Nalpha^2 \times (\Norb - \Nalpha)^2}$.  One possibility
would be to distribute the storage of Hamiltonian among multiple compute nodes,
and use a distributed library such as PBLAS\cite{pblas} to perform the
matrix-vector operations. Another approach is to use a so-called \emph{direct}
algorithm, where the matrix elements are computed \emph{on the fly}, and this
is the approach chosen in the \QP.


This effectively means iterating over all pairs of determinants $\kI$ an
$\kJ$, checking whether $\kI$ and $\kJ$ are connected by $\mH$ and if they are,
accessing the corresponding integral(s) and computing the phase factor. Even
though we presented in section~\ref{sec:det_exc} a very efficient method to
compute the excitation degree
between two determinants, the number of such computations to be made scales as
$\Ndet^2$, which can still be prohibitively high. To get an efficient
implementation it is mandatory to filter out all pairs of determinants that are
not connected by $\mH$, and iterate only over connected pairs. To reach this
goal, we have implemented an algorithm similar to the \emph{Direct Selected
Configuration Interaction Using Strings} (DISCIUS)
algorithm.\cite{Povill_1995}

The finality is to build the matrix $\mW$ as
\begin{equation}
W_{Ik} = \sum_J H_{IJ}\, U_{Jk}.
\label{eq:whu}
\end{equation}
The diagonal of $\mH$ is precomputed and stored in a one-dimensional array, as it is
used both for the calculation of $\mW$ and the residual $\mR$.
$\mW$ is initialized with $W_{Ik} = H_{II}\, U_{Ik}$, and
each time a connected pair of determinants $(I, J \neq I)$ is found, the $I$-th component of all states $k$ stored in $\mW$ is updated accordingly. To make this step efficient memory-wise, $\mU$ and $\mW$ are stored transposed, such that the state indices $k$ are contiguous in memory.

We present our algorithm for iterating over only off-diagonal non-zero elements
of $\mH$ ---~in other words, pairs of connected determinants~--- as
algorithms~\ref{alg:davidson_aa} and~\ref{alg:davidson_ab}.

\begin{algorithm}
\caption{Find internal determinants connected by purely $\uparrow$ or purely $\downarrow$ single or double excitations}
\label{alg:davidson_aa}
\KwData{$\Nalphadet$ the number of unique $\uparrow$ spin parts present in $\ket \Psi$}
\KwData{$D$ is the array of determinants present in $\ket \Psi$, sorted by $\uparrow$-major order (all determinants sharing the same $\uparrow$ part are next to each other)}
\KwData{$A$ the array so that $A[n]$ is the index of the first occurence of the $n^{th}$ unique $\uparrow$ spin part in $D$. For algorithmic convenience we set $A[\Nalphadet +1] = \Ndet+1$}

\For{$a \gets 1, \Nalphadet$}{
\tcc{All determinants sharing $D[A(a)]_\uparrow$ $\uparrow$-spin part are in the range $[A(a), A(a+1)-1]$}
\For{$b1 \gets A(a), A(a+1)-1$}{
\For{$b2 \gets b1+1, A(a+1)-1$}{
	\If{$\text{EXC\_DEGREE}(D[b1]_\downarrow, D[b2]_\downarrow) \leq 2$}{
		$\ket {D[b1]}$ connected to $\ket {D[b2]}$ by single or double $\downarrow$ excitation.	
	}
}
}
}
\tcc{Single and double $\uparrow$ excitations are found by the same algorithm after flipping spins}
\end{algorithm}


\begin{algorithm}
\caption{Find internal determinants connected by $\uparrow\downarrow$ double excitations}
\label{alg:davidson_ab}
\KwData{see algorithm \ref{alg:davidson_aa}}

\For{$a1 \gets 1, \Nalphadet$}{
\For{$a2 \gets a1+1, \Nalphadet$}{
\If{$\text{EXC\_DEGREE}(D[A(a1)]_\uparrow, D[A(a2)]_\uparrow) \neq 1$}{
	cycle $a2$ loop\;
}
\For{$b1 \gets A(a1), A(a1+1)-1$}{
\For{$b2 \gets A(a2), A(a2+1)-1$}{
	\If{$\text{EXC\_DEGREE}(D[b1]_\downarrow, D[b2]_\downarrow) = 1$}{
		$\ket {D[b1]}$ connected to $\ket {D[b2]}$ by $\uparrow\downarrow$ excitation.
	}
}
}
}
}
\end{algorithm}

We define $\Nalphadet$ and $\Nbetadet$ as the number of different $\uparrow$ and $\downarrow$
spin parts present in the expression of the wavefunction.
Computing the contributions the same-spin single and double excitations
(algorithm~\ref{alg:davidson_aa}) scales as $\order{\Nalphadet^2}$, which is equal to
$\order{\Ndet}$. Indeed, when the FCI is reached $\Ndet = \Nalphadet \times \Nbetadet$.

The $\uparrow\downarrow$ double excitations are the most expensive (algorithm~\ref{alg:davidson_ab})
as they scale as~$\order{\Nalphadet \Nbetadet^2} = \order{\Nalphadet^3}$. Indeed, the \texttt{cycle} instruction at line~4 makes the
iterations over $b1$ and $b2$ do the computation only a number of times bounded by the number of 
possible single excitations ($\Nalpha \times \qty(\Norb - \Nalpha)$). So at the FCI level, this step scales as
$\order{\Ndet^{3/2}}$.

One can remark that during the computation of the contributions of the single
excitations, one can store the lists of all singly-excited determinants
for all $\uparrow$ and $\downarrow$ spin parts.
These lists can be reused in the computation of the contributions of the $\uparrow\downarrow$
double excitations, so as to loop only over the single excitations on
$D_\uparrow$, and on the single excitations on $D_\downarrow$. As the lengths of the
lists of single excitations are bounded by $\Nalpha \times \qty(\Norb - \Nalpha)$, the
algorithm then scales linearly with $\Ndet$.  However, in practice the CIPSI
selection produces wave functions where $\Nalphadet$ and $\Nbetadet$ are much larger
that $\Ndet^{1/2}$, and the storage of the single excitations can become a
memory bottleneck that we want to avoid at the cost of more computation.

\section{Sorting}

The presented algorithm requires to sort the determinants by $\uparrow$-major order~: all determinants
sharing the same $\uparrow$ spin part next to each other. To perform this sort, we simply consider the
bitstring representation of the determinants as tuples of integers and sort the list of tuples.

Surprisingly, the sort can be done in $\order{\Ndet}$ instead of $\order{\Ndet \log(\Ndet)}$ with the
radix sort algorithm.\cite{Davis1992Dec}
The principle of the radix sort is presented in algorithm~\ref{alg:radix}.  The
key feature enabling the transition from $\order{\Ndet}$ to $\order{\Ndet
\log(\Ndet)}$ is the fact that the set of sorted integers is bounded, and one
can easily verify that the number of operations is proportional to $64\times
\Ndet$. So sorting the determinants of the wave function is not a bottleneck,
and the flop-optimal algorithm still scales as $\order{\Ndet}$.

\begin{algorithm}
\caption{Radix sort algorithm for non-negative integers}
\label{alg:radix}
\SetKwFunction{FMain}{RADIX\_SORT} 
\SetKwProg{Fn}{Function}{:}{}
\Fn(){\FMain{$D,N$}}{
 \KwData{$D$: Array of integers to sort in input, sorted in output}
 \KwData{$N$: Length of the array $D$}
 RADIX\_SORT\_rec($D,N,64$) \;
}

\SetKwFunction{FMain}{RADIX\_SORT\_rec} 
\SetKwProg{Fn}{Function}{:}{}
\Fn(){\FMain{$D,N,i$}}{
 \KwData{$D$: Array of integers to sort in input, sorted in output}
 \KwData{$N$: Length of the array $D$}
 \KwData{$r$: index of the inspected bit}

 \If{$r \ge 0$}{

  Allocate temporary arrays $D_0(1:N)$ and $D_1(1:N)$\;
  $l\gets 1$ \; $r\gets 1$ \;
  \For{$k \gets 1, N$}{

   \uIf{$\btest{D(k)}{i}$}{
    $D_1[l] \gets D[k]$ \;
    $l \gets l+1$ \;
   }
   \Else {
     $D_0[r] \gets D[k]$ \;
     $r \gets r+1$ \;
   }
  }

  RADIX\_SORT\_rec($D_0,r,i-1$) \;
  RADIX\_SORT\_rec($D_1,l,i-1$) \;
  \For{$k \gets 1, l$}{
    $D(k) \gets D_0(k)$ \;
  }
  $r \gets 1$ \;
  \For{$k \gets l+1, N$}{
    $D(k) \gets D_1(r)$ \;
    $r \gets r+1$ \;
  }
 }
}
\end{algorithm}




\section{Parallelization}

To minimize the network communication, we separate the calculation in tasks
such that the tasks build disjoint pieces of the result. A task corresponds a
range of indices $I$ in Eq~\eqref{eq:whu}. Therefore, the communication for the
result is $\order{\Ndet}$, and independent of the number of compute nodes.
However, each task needs the complete $\mU$ matrix, so its needs to be
broadcast efficiently on every compute node at the beginning of the
calculation. This broadcast if performed via an MPI library call for optimal
performance,\cite{MPI} and we use one MPI process per node such that the amount of
communication scales with the number of nodes and not with the number of cores.

\begin{algorithm}
\caption{Find internal determinants connected by $\uparrow\downarrow$ double excitations, one in the range $[\text{first}, \text{last}]$ the other in the range $[\text{first}, \Ndet]$}
\label{alg:davidson_ab_parallel}
\KwData{see algorithm \ref{alg:davidson_aa}}
\KwData{$\text{first}$, $\text{last}$ : the boundaries of the range of determinants (in $D$) processed by the current OpenMP thread.}
\For{$a1 \gets 1, \Nalphadet$}{
\If{$A(a1+1) -1 < \text{first}$}{
	cycle $a1$ loop\;
}
\If{$A(a1) > \text{last}$)}{
	return \;
}
$f \gets \max(\text{first}, A(a1))$ \;
$t \gets \min(\text{last}, A(a1+1)-1)$ \;
\For{$a2 \gets 1, \Nalphadet$}{
\If{$\text{EXC\_DEGREE}(D[A(a1)]_\uparrow, D[A(a2)]_\uparrow) \neq 1$}{
	cycle $a2$ loop\;
}
\For{$b1 \gets f,t$}{
\For{$b2 \gets A(a2), A(a2+1)-1$}{
	\If{$\text{EXC\_DEGREE}(D[b1]_\downarrow, D[b2]_\downarrow) = 1$}{
		$\ket {D[b1]}$ connected to $\ket {D[b2]}$ by $\uparrow\downarrow$ excitation.	
	}
}
}
}
}
\end{algorithm}

When idle, an MPI process requests a task to the server, and computes the
corresponding result in parallel with OpenMP.\cite{openmp} This allows
the sharing of the $\mU$ matrix, as well as the result array for $\mW$,
but also of all the large constant data needed for the calculation, such as
the two-electron integrals.
The OpenMP parallelization is made on the outermost loop, so each OpenMP thread
loops over a smaller range of $I$ (algorithm~\ref{alg:davidson_ab_parallel}).
The write
access to the result is guaranteed to be safe, without requiring a lock.
As the OpenMP tasks are not guaranteed to be balanced, we have used a
dynamic scheduling, with a chunk size of 64 elements. The reason for this
chunk size is to force that multiple threads accumulate their results in 
memory addresses far apart, avoiding the so-called \emph{false sharing}
performance degradation that occurs when multiple threads write simultaneously
in the same cache line.\cite{Bolosky1993Sep} 
When the result is fully computed, it is sent back to the master process and
a new task is requested, until the task queue is empty.


\section{Symmetry of $\mH$}


Taking into account the symmetry of $\widehat H$, each pair should be found only once, and the associated update would be
\begin{align}
W_{Ik} \gets W_{Ik} + U_{Jk} H_{IJ} \\
W_{Jk} \gets W_{Jk} + U_{Ik} H_{IJ}
\end{align}
This reduces the computational effort by a factor of two, but the result of
each task now has a size of $\Ndet$ and no more the reduced size of $\Ndet / N_\text{task}$,
since all the elements of $\mW$ can potentially be modified.
This increase of communication has the effect of killing the parallel
efficiency.

There are some additional drawbacks.
First, in the non-symmetric case, a thread accumulates data in $W_{Ik}$, a memory
location which is the same for multiple consecutive accesses, and in which no other
thread can write. This pattern is memory-efficient.
In the symmetric case, there is also an access to $W_{Jk}$. The access to
$W_{Jk}$ are non-contiguous and don't have a predictable pattern by the hardware.
Such memory access patterns are terribly inefficient, especially when writing.
In addition, a global memory lock should be acquired since there is no
guarantee than another thread is not writing in that memory location at the
same time. To avoid the lock, another solution is to use an output vector which
is private to the thread, but it would make the memory grow as $N_\text{CPU}
\times \Nst \times \Ndet$, which is what
we wanted to avoid using shared memory parallelism.

For a large number of nodes it is indisputably preferable not to use the
symmetry of $\mH$, even though it might seem surprising that increasing the
number of operations can give a better time to solution.

%\paragraph{Taking the symmetry of $\widehat H$ into account}
%A task will correspond to some set of determinants $\{\mathcal{I}\}$ for which connections only to ``later'' determinants must be found, to avoid double finding a connected pair. Each found connection will involve $\ket {D_{I \in \mathcal{I}}}$ and $\ket {D_{J>I}}$, and causes an increment to $W_{I \in \mathcal{I}}$ and $W_{J>I}$. The result for a task is a vector size up to $\Ndet$.
%
%\paragraph{Not taking the symmetry of $\widehat H$ into account}
%In this case, all connections involving determinants of $\{\mathcal{I}\}$ must be found. Each found connection involves $\ket {D_{I \in \mathcal{I}}}$ and $\ket {D_{J}}$. But since we can only take into account the increment to $W_{I \in \mathcal{I}}$, the result is a vector size $|\mathcal{I}|$ the number of elements in $\{\mathcal{I}\}$, which is much smaller than $\Ndet$.
%
%\paragraph{}
%Even though the later approach doubles the amount of work, it drastically reduces the amount of data transfer in our task-result parallel scheme, therefore it is the one currently implemented.



\section{Ensuring the solutions have the desired spin state}

When working in a truncated space of determinants, there is no guarantee that the eigenstate of $\hat H$ will also be eigenstates of the spin operator $\hat S^2$. And when the proper conditions are fulfilled (see section~\ref{sec:cipsi_s2}), all the lowest eigenvectors may be of different spin states.

To help find solutions of the desired spin state, we use a penalty method in the diagonalization.\cite{Fales2017Sep} We modify the Hamiltonian as 
\begin{equation}
\tilde{\mH} = \mH + \gamma \qty( \mS^2 - \mathbf{I} \langle S^2 \rangle_\text{target} )^2
\end{equation}
where $\gamma$ is a fixed parameter.
In the Davidson algorithm, this requires the additional computation of $\mS\, \mU$, for which the
cost is expected to be the same as the cost of $\mH \, \mU$ as the expensive part is the search for the
connections.

We have modified the function computing $\mH \, \mU$ so that it also computes $\mS\, \mU$ on the fly.
Indeed, once a pair of connected determinants has been found, if they correspond to an $\uparrow \downarrow$ double excitation or to a diagonal term, the $\hat S^2$ contribution is added to an extra output vector,
with almost no extra computational cost.

\section{Summary}

Davidson's diagonalization algorithm was implemented in its multi-state
version. A direct algorithm was designed for arbitrary sets of determinants, and the
formal scaling was reduced to $\order{\Ndet^{3/2}}$, and can be further reduced to $\order{\Ndet}$
at the cost of additional storage. The implementation was parallelized
using two levels of parallelism, MPI and OpenMP, keeping in mind the reduction
of the communication. The empirical speedup measurements are presented in
chapter~\ref{chap:PERF}.



\end{document}
